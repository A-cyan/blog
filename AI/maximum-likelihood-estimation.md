# 最大似然估计

原文链接： [https://lulaoshi.info/machine-learning/linear-model/maximum-likelihood-estimation](https://lulaoshi.info/machine-learning/linear-model/maximum-likelihood-estimation)

前文 [从线性回归走进机器学习](AI/linear-regression.md) 介绍了线性回归以及最小二乘法的数学推导过程。

对于一组训练数据，使用线性回归建模，可以有不同的模型参数来描述数据，这时候可以用最小二乘法来选择最优参数来拟合训练数据，即使用误差的平方作为损失函数。

机器学习求解参数的过程被称为参数估计，机器学习问题也变成求使损失函数最小的最优化问题。

最小二乘法比较直观，很容易解释，但不具有普遍意义，对于更多其他机器学习问题，比如二分类和多分类问题，最小二乘法就难以派上用场了。本文将给大家介绍一个具有普遍意义的参数估计方法：`最大似然估计`。

## 概率和似然

下面以一个猜硬币的例子来模拟机器学习的概率推理过程。

假设你被告知一个硬币抛掷10次的正反情况，接下来由你来猜，而你只有一次机会，猜对硬币下一次正反情况则赢得100元，猜错则损失100元。

这时，你会如何决策？

一般地，硬币有正反两面，如果硬币正反两面是均匀的，即每次抛掷后硬币为正的概率是0.5。使用这个硬币，很可能抛10次，有5次是正面。

但是假如有人对硬币做了手脚，比如提前对硬币做了修改，硬币每次都会正面朝上，现在抛10次，10次都是正面，那么下次你绝对不会猜它是反面，因为前面的10次结果摆在那里，直觉上你不会相信这是一个普通的硬币。

现在有一人抛了10次硬币，得到6正4反的结果，如何估算下次硬币为正的概率呢？

因为硬币并不是我们制作的，我们不了解硬币是否是完全均匀的，只能根据现在的观察结果来反推硬币的情况。

假设硬币上有个参数`θ`，它决定了硬币的正反均匀程度，`θ=0.5`表示正反均匀，每次抛硬币为正的概率为0.5，`θ=1.0`表示硬币只有正面，每次抛硬币为正的概率为1。

那么，从观察到的正反结果，反推硬币的构造参数`θ`的过程，就是一个参数估计的过程。

### 概率

抛掷10次硬币可能出现不同的情况，可以是“5正5反”、“4正6反”，“10正0反”等。

假如我们知道硬币是如何构造的，即已知硬币的参数$\theta$，那么出现“6正4反”的概率为：

$$
P(6正4反 \ |\ \theta=0.5)=C_{10}^{6}\times 0.5^6 \times (1-0.5)^4 \approx 0.2051 \\
P(6正4反 \ |\ \theta=0.6)=C_{10}^{6}\times 0.6^6 \times (1-0.6)^4 \approx 0.2508 \\
P(6正4反 \ |\ \theta=0.9)=C_{10}^{6}\times 0.9^6 \times (1-0.9)^4 \approx 0.0112
$$

详细解释：

1. 上面这个公式是概率函数，表示已知参数$\theta$，事实“6正4反”发生的概率。

2. 参数$\theta$取不同的值时，事情发生的概率不同。在数学上一般使用$P$或$Pr$表示概率（Probability）函数。

3. 上述过程中，抛10次硬币，要选出6次正面，使用了排列组合。因为“6正4反”可能会出现`正正正正正正反反反反`、`正正正正正反正反反反`、`正正正正反正正反反反`等共210种组合，要在10次中选出6次为正面。假如每次正面的概率是0.6，那么反面的概率就是(1-0.6)。每次抛掷硬币的动作是相互独立，互不影响的，“6正4反”发生的概率就是各次抛掷硬币的概率乘积，再乘以210种组合。

概率反映的是：**已知背后原因，推测某个结果发生的概率**。

### 似然

与概率不同，似然反映的是：**已知结果，反推原因**。

具体而言，似然（`Likelihood`）函数表示的是基于观察的数据，取不同的参数$\theta$时，统计模型以多大的可能性接近真实观察数据。

这就很像前面举的例子，已经给你了一系列硬币正反情况，但你并不知道硬币的构造，下次下注时你要根据已有事实，反推硬币的构造。

例如，当观察到硬币“10正0反”的事实，猜测硬币极有可能每次都是正面；当观察到硬币“6正4反”的事实，猜测硬币有可能不是正反均匀的，每次出现正面的可能性是0.6。

似然函数与前面的概率函数的计算方式极其相似，与概率函数不同的是，似然函数是$\theta$的函数，即$\theta$是未知的。

似然函数衡量的是在不同参数$\theta$下，真实观察数据发生的可能性。

似然函数通常是多个观测数据发生的概率的联合概率，即多个观测数据都发生的概率。

在机器学习里可以这样理解，目标$y$和特征$\boldsymbol{x}$同时发生，这些数值被观测到的概率。

单个观测数据发生的可能性为$P(\theta)$，如果各个观测之间是相互独立的，那么多个观测数据都发生的概率可表示为**各个样本发生的概率的乘积**。

这里稍微解释一下事件独立性与联合概率之间的关系。

如果事件A和事件B相互独立，那么事件A和B同时发生的概率是 $ P(A) \times P(B)$ 。

例如：

- 事件“下雨”与事件“地面湿”就不是相互独立的，“下雨”与"地面湿"是同时发生、高度相关的，这两个事件都发生的概率就不能用概率的乘积来表示。
- 两次抛掷硬币相互之间不影响，因此硬币正面朝上的概率可以用各次概率的乘积来表示。

似然函数通常用$L$表示，对应英文`Likelihood`。观察到抛硬币“6正4反”的事实，硬币参数$\theta$取不同值时，似然函数表示为：

$$
L(\theta ; 6正4反)=C_{10}^{6}\times \theta^6 \times (1-\theta)^4 \\
$$

这个公式的图形如下图所示。

![](http://img2020.cnblogs.com/blog/1546632/202110/1546632-20211026143124935-2014252115.png)

从图中可以看出：参数$\theta$为0.6时，似然函数最大，参数为其他值时，“6正4反”发生的概率都相对更小。

所以猜测下次硬币为正，因为根据已有观察，硬币很可能以0.6的概率为正。

推广到更为一般的场景，似然函数的一般形式可以用下面公式来表示，也就是之前提到的，各个样本发生的概率的乘积。

$$
L(\theta ; \mathbf{X}) = P_1(\theta ; X_1) \times P_2(\theta ; X_2) ... \times P_n(\theta ; X_n) = \prod P_i(\theta; X_i)
$$

## 最大似然估计

理解了似然函数的含义，就很容易理解最大似然估计的机制。

似然函数是关于模型参数的函数，是描述观察到的真实数据在不同参数下发生的概率。

最大似然估计要寻找最优参数，让似然函数最大化。或者说，使用最优参数时观测数据发生的概率最大。

### 线性回归的最大似然估计

之前的文章提到，线性回归的误差项$ε$是预测值与真实值之间的差异，如下面公式所示。它可能是一些随机噪音，也可能是线性回归模型没考虑到的一些其他影响因素。
$$
y^{(i)} = \epsilon^{(i)} + \sum_{j=1}^n w_jx_{j}^{(i)} =\epsilon^{(i)} + \boldsymbol{w}^\top \boldsymbol{x}^{(i)}
$$

